"""Step 4: Preprocessing - Prepare parsed content for chunking using LLM.

This module handles document preprocessing:
1. LLM-based agenda item structure injection
2. Header normalization (#, ##) for Parent-Child chunking
3. Content cleanup and formatting
"""

from dataclasses import dataclass

import google.generativeai as genai
import structlog

from app.core.config import settings

logger = structlog.get_logger()

# Configure Gemini
genai.configure(api_key=settings.GEMINI_API_KEY)


# LLM Prompt for preprocessing meeting documents
PREPROCESSING_PROMPT = """ë‹¹ì‹ ì€ ì„œìš¸ëŒ€í•™êµ ì»´í“¨í„°ê³µí•™ë¶€ í•™ìƒíšŒ íšŒì˜ë¡ êµ¬ì¡°í™” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ì•„ë˜ íšŒì˜ ë¬¸ì„œë¥¼ ë¶„ì„í•˜ì—¬ Markdown í—¤ë” êµ¬ì¡°ë¥¼ ì¶”ê°€í•´ì£¼ì„¸ìš”.

## ğŸš¨ ìµœìš°ì„  ê·œì¹™ (ë°˜ë“œì‹œ ì¤€ìˆ˜):
1. **ì›ë³¸ í…ìŠ¤íŠ¸ë¥¼ 100% ìœ ì§€í•˜ì„¸ìš”**
   - ëª¨ë“  ë¬¸ì¥, ëª¨ë“  ë‹¨ì–´, ëª¨ë“  í‘œ, ëª¨ë“  ë¦¬ìŠ¤íŠ¸ë¥¼ ê·¸ëŒ€ë¡œ ì¶œë ¥
   - ì–´ë–¤ ë‚´ìš©ë„ ìƒëµ, ìš”ì•½, ì¶•ì•½ ì ˆëŒ€ ê¸ˆì§€
   - ì…ë ¥ ë¬¸ì„œ ê¸¸ì´ì™€ ì¶œë ¥ ë¬¸ì„œ ê¸¸ì´ê°€ ê±°ì˜ ë™ì¼í•´ì•¼ í•¨
   
2. **í—¤ë”ë§Œ ì‚½ì…**
   - ì˜¤ì§ `#`ì™€ `##` í—¤ë”ë§Œ ì ì ˆí•œ ìœ„ì¹˜ì— ì¶”ê°€
   - ì›ë³¸ì˜ ì¤„ë°”ê¿ˆ, ê³µë°±ë„ ìœ ì§€

3. **ê¸°ì¡´ `#`, `##` í—¤ë”ëŠ” ë¬´ì‹œ**
   - íŒŒì„œê°€ ì„ì˜ë¡œ ìƒì„±í•œ í—¤ë”ì´ë¯€ë¡œ ì œê±°í•˜ê³  ìƒˆë¡œ êµ¬ì¡°í™”

## ğŸ“‹ í—¤ë” êµ¬ì¡° ê·œì¹™:
1. **ì•ˆê±´ ì¢…ë¥˜**ëŠ” `#` (H1) í—¤ë”ë¡œ í‘œì‹œ
   - ì˜ˆ: `# ë³´ê³ ì•ˆê±´`, `# ë…¼ì˜ì•ˆê±´`, `# ê¸°íƒ€ì•ˆê±´`
   
2. **ê°œë³„ ì•ˆê±´**ì€ `##` (H2) í—¤ë”ë¡œ í‘œì‹œ
   - ì˜ˆ: `## ë³´ê³ ì•ˆê±´ 1. í•™ìƒíšŒì¥ë‹¨ í™œë™ë³´ê³ `
   - ì˜ˆ: `## ë…¼ì˜ì•ˆê±´ 2. 2025 ì»´ë°¤, ì»´ë‚®`

## ğŸ” ì•ˆê±´ íŒŒì•… ë°©ë²•:
- **ë¬¸ì„œ ìƒë‹¨ì— ì•ˆê±´ ìš”ì•½í‘œê°€ ìˆìŠµë‹ˆë‹¤** (í•­ìƒ ì¡´ì¬)
- í˜•ì‹: `| ì•ˆê±´ | <ë³´ê³ ì•ˆê±´> 1. ì œëª© 2. ì œëª© <ë…¼ì˜ì•ˆê±´> 1. ì œëª© ... |`
- ì´ ìš”ì•½í‘œë¥¼ ì°¸ê³ í•˜ì—¬ ë³¸ë¬¸ì˜ ê° ì•ˆê±´ ì‹œì‘ ìœ„ì¹˜ì— í—¤ë”ë¥¼ ì‚½ì…

## ì¶œë ¥:
- Markdown í˜•ì‹
- ì›ë³¸ ë‚´ìš© 100% í¬í•¨ + í—¤ë” êµ¬ì¡°ë§Œ ì¶”ê°€
- ì„¤ëª…ì´ë‚˜ ì£¼ì„ ì—†ì´ ê²°ê³¼ë¬¼ë§Œ ì¶œë ¥

## ì…ë ¥ ë¬¸ì„œ:
{content}

---
ìœ„ ë¬¸ì„œë¥¼ ì•ˆê±´ ê¸°ì¤€ìœ¼ë¡œ í—¤ë” êµ¬ì¡°í™”í•˜ì—¬ **ì „ì²´ ë‚´ìš©ì„ ìœ ì§€í•˜ë©°** ì¶œë ¥í•˜ì„¸ìš”."""


# Prompt for non-meeting documents (simpler structure)
SIMPLE_PREPROCESSING_PROMPT = """ì•„ë˜ ë¬¸ì„œì˜ ë‚´ìš©ì„ ì •ë¦¬í•˜ì—¬ Markdown í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•´ì£¼ì„¸ìš”.

## âš ï¸ ì ˆëŒ€ ê·œì¹™:
- **ì›ë³¸ í…ìŠ¤íŠ¸ë¥¼ í•œ ê¸€ìë„ ìˆ˜ì •í•˜ì§€ ë§ˆì„¸ìš”**
- ë‚´ìš© ìš”ì•½, ìƒëµ, ì¬ì‘ì„± ê¸ˆì§€
- ê¸°ì¡´ `#`, `##` í—¤ë”ëŠ” ì œê±°í•˜ê³  ìƒˆë¡œ êµ¬ì¡°í™”

## ê·œì¹™:
1. ì£¼ìš” ì„¹ì…˜ì€ `##` í—¤ë”ë¡œ í‘œì‹œ
2. ê¸°ì¡´ êµ¬ì¡°ì™€ ë‚´ìš© ìœ ì§€
3. ë¶ˆí•„ìš”í•œ ê³µë°±ì´ë‚˜ ì„œì‹ë§Œ ì •ë¦¬

## ì…ë ¥ ë¬¸ì„œ:
{content}

---
Markdownìœ¼ë¡œ ë³€í™˜ëœ ë¬¸ì„œë§Œ ì¶œë ¥í•˜ì„¸ìš”."""


@dataclass
class PreprocessingResult:
    """Result of document preprocessing."""
    
    processed_content: str
    original_content: str
    headers_found: list[str]
    sections_count: int
    is_meeting_document: bool


class PreprocessingService:
    """
    Service for preprocessing parsed documents before chunking.
    
    Uses Gemini 2.0 Flash to inject proper Markdown header structure
    for Parent-Child chunking strategy.
    
    Meeting documents get special treatment:
        - # for agenda types (ë³´ê³ ì•ˆê±´, ë…¼ì˜ì•ˆê±´, ì˜ê²°ì•ˆê±´)
        - ## for individual agenda items
    """

    def __init__(self):
        """Initialize preprocessing service."""
        # Increase max_output_tokens to prevent content truncation
        self.model = genai.GenerativeModel(
            settings.GEMINI_MODEL,
            generation_config=genai.GenerationConfig(
                max_output_tokens=32000,  # Max for Gemini 2.0 Flash
                temperature=0.1,  # Low temperature for more faithful output
            ),
        )

    async def preprocess_document(
        self,
        content: str,
        is_meeting_document: bool = True,
        document_type: str | None = None,
    ) -> PreprocessingResult:
        """
        Preprocess a parsed document for chunking.
        
        Args:
            content: Parsed document content (HTML or text)
            is_meeting_document: Whether this is a meeting document
            document_type: Specific document type (agenda, minutes, result)
            
        Returns:
            PreprocessingResult with structured Markdown content
        """
        # Choose appropriate prompt based on document type
        if is_meeting_document:
            prompt = PREPROCESSING_PROMPT.format(content=content)
        else:
            prompt = SIMPLE_PREPROCESSING_PROMPT.format(content=content)

        try:
            response = await self.model.generate_content_async(prompt)
            processed_content = response.text.strip()
            
            # Clean up any markdown code blocks from LLM response
            if processed_content.startswith("```"):
                lines = processed_content.split("\n")
                # Remove first and last lines if they're code block markers
                if lines[0].startswith("```"):
                    lines = lines[1:]
                if lines and lines[-1].strip() == "```":
                    lines = lines[:-1]
                processed_content = "\n".join(lines)

            # Log content length comparison to detect loss
            original_len = len(content)
            processed_len = len(processed_content)
            loss_ratio = 1 - (processed_len / original_len) if original_len > 0 else 0
            
            logger.info(
                "Preprocessing complete",
                original_length=original_len,
                processed_length=processed_len,
                loss_ratio=f"{loss_ratio:.1%}",
            )
            
            # If severe content loss (>50%), fall back to original with basic cleanup
            if loss_ratio > 0.5:
                logger.warning(
                    "Severe content loss detected, using original content",
                    loss_ratio=f"{loss_ratio:.1%}",
                )
                processed_content = self._basic_cleanup(content)

            # Extract headers for metadata
            headers = self._extract_headers(processed_content)
            sections_count = len([h for h in headers if h.startswith("## ")])

            return PreprocessingResult(
                processed_content=processed_content,
                original_content=content,
                headers_found=headers,
                sections_count=sections_count,
                is_meeting_document=is_meeting_document,
            )

        except Exception as e:
            logger.error("Preprocessing failed", error=str(e))
            # Fall back to original content with basic cleanup
            return PreprocessingResult(
                processed_content=self._basic_cleanup(content),
                original_content=content,
                headers_found=[],
                sections_count=0,
                is_meeting_document=is_meeting_document,
            )

    def _extract_headers(self, content: str) -> list[str]:
        """Extract all Markdown headers from content."""
        import re
        pattern = r'^(#{1,3})\s+(.+)$'
        headers = []
        for match in re.finditer(pattern, content, re.MULTILINE):
            headers.append(f"{match.group(1)} {match.group(2)}")
        return headers

    def _basic_cleanup(self, content: str) -> str:
        """
        Basic cleanup of content without LLM processing.
        
        Removes excessive whitespace and normalizes line breaks.
        """
        import re
        
        # Normalize line breaks
        content = content.replace("\r\n", "\n").replace("\r", "\n")
        
        # Remove excessive blank lines (more than 2 consecutive)
        content = re.sub(r'\n{3,}', '\n\n', content)
        
        # Strip trailing whitespace from lines
        lines = [line.rstrip() for line in content.split('\n')]
        content = '\n'.join(lines)
        
        return content.strip()

    async def extract_agenda_summary(
        self,
        content: str,
    ) -> dict[str, list[str]]:
        """
        Extract agenda summary from meeting document.
        
        Useful for building event timeline data.
        
        Args:
            content: Document content
            
        Returns:
            Dictionary with agenda types as keys and item lists as values
        """
        prompt = """ì•„ë˜ íšŒì˜ ë¬¸ì„œì—ì„œ ì•ˆê±´ ëª©ë¡ì„ ì¶”ì¶œí•´ì£¼ì„¸ìš”.

## ì¶œë ¥ í˜•ì‹ (JSON):
{
    "ë³´ê³ ì•ˆê±´": ["ì•ˆê±´1", "ì•ˆê±´2"],
    "ë…¼ì˜ì•ˆê±´": ["ì•ˆê±´1", "ì•ˆê±´2"],
    "ì˜ê²°ì•ˆê±´": ["ì•ˆê±´1", "ì•ˆê±´2"]
}

ì—†ëŠ” ì¢…ë¥˜ëŠ” ë¹ˆ ë°°ì—´ë¡œ í‘œì‹œí•˜ì„¸ìš”.

## ë¬¸ì„œ:
{content}

JSONë§Œ ì¶œë ¥í•˜ì„¸ìš”.""".format(content=content[:5000])  # Limit content length

        try:
            import json
            response = await self.model.generate_content_async(prompt)
            result_text = response.text.strip()
            
            # Clean up JSON response
            if result_text.startswith("```"):
                result_text = result_text.split("```")[1]
                if result_text.startswith("json"):
                    result_text = result_text[4:]
            
            return json.loads(result_text)
            
        except Exception as e:
            logger.warning("Agenda extraction failed", error=str(e))
            return {"ë³´ê³ ì•ˆê±´": [], "ë…¼ì˜ì•ˆê±´": [], "ì˜ê²°ì•ˆê±´": []}

    async def extract_decisions(
        self,
        content: str,
    ) -> list[dict[str, str]]:
        """
        Extract decisions/action items from meeting result document.
        
        Args:
            content: Document content
            
        Returns:
            List of decision dictionaries
        """
        prompt = """ì•„ë˜ íšŒì˜ ê²°ê³¼ ë¬¸ì„œì—ì„œ ê²°ì • ì‚¬í•­ê³¼ ì•¡ì…˜ ì•„ì´í…œì„ ì¶”ì¶œí•´ì£¼ì„¸ìš”.

## ì¶œë ¥ í˜•ì‹ (JSON):
[
    {
        "agenda_item": "ì•ˆê±´ëª…",
        "decision": "ê²°ì • ë‚´ìš©",
        "assignee": "ë‹´ë‹¹ì (ì—†ìœ¼ë©´ null)",
        "deadline": "ë§ˆê°ì¼ (ì—†ìœ¼ë©´ null)"
    }
]

## ë¬¸ì„œ:
{content}

JSONë§Œ ì¶œë ¥í•˜ì„¸ìš”.""".format(content=content[:5000])

        try:
            import json
            response = await self.model.generate_content_async(prompt)
            result_text = response.text.strip()
            
            if result_text.startswith("```"):
                result_text = result_text.split("```")[1]
                if result_text.startswith("json"):
                    result_text = result_text[4:]
            
            return json.loads(result_text)
            
        except Exception as e:
            logger.warning("Decision extraction failed", error=str(e))
            return []
