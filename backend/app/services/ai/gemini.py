"""Gemini AI service for LLM and Vision operations."""

import base64
import json
from typing import Any

import google.generativeai as genai
import structlog

from app.core.config import settings

logger = structlog.get_logger()


class GeminiService:
    """Service for Gemini LLM and Vision capabilities."""

    def __init__(self) -> None:
        # Vertex AIê°€ ì•„ë‹Œ Google AI Studio API í‚¤ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš° configure í•„ìš”
        # Vertex AI í™˜ê²½(GCP)ì´ë¼ë©´ ì´ˆê¸°í™” ë°©ì‹ì´ ë‹¤ë¥¼ ìˆ˜ ìˆìœ¼ë‚˜,
        # í˜„ì¬ ì½”ë“œ ë² ì´ìŠ¤ëŠ” api_key ë°©ì‹ì„ ë”°ë¦…ë‹ˆë‹¤.
        genai.configure(api_key=settings.GEMINI_API_KEY)
        self._model = None
        self._vision_model = None

        # ğŸš€ [Upgrade] ìµœì‹  Gemini 2.0 ëª¨ë¸ ì‚¬ìš©
        # ë§Œì•½ ì—ëŸ¬ ë°œìƒ ì‹œ "gemini-1.5-flash-001"ë¡œ ë³€ê²½í•˜ì„¸ìš”.
        self.MODEL_NAME = "gemini-flash-latest"

    @property
    def model(self):
        """Get text generation model."""
        if self._model is None:
            self._model = genai.GenerativeModel(self.MODEL_NAME)
        return self._model

    @property
    def vision_model(self):
        """Get vision-capable model."""
        if self._vision_model is None:
            self._vision_model = genai.GenerativeModel(self.MODEL_NAME)
        return self._vision_model

    def _parse_json_response(self, response_text: str) -> dict | list:
        """Helper to cleanly parse JSON from LLM response."""
        try:
            json_str = response_text
            if "```json" in response_text:
                json_str = response_text.split("```json")[1].split("```")[0]
            elif "```" in response_text:
                json_str = response_text.split("```")[1].split("```")[0]

            return json.loads(json_str.strip())
        except (json.JSONDecodeError, IndexError):
            # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì›ë³¸ í…ìŠ¤íŠ¸ë¥¼ í¬í•¨í•œ ì—ëŸ¬ êµ¬ì¡° ë°˜í™˜ ë˜ëŠ” ë¹ˆ ê°’ ë°˜í™˜
            return {}

    def generate_text(
        self,
        prompt: str,
        temperature: float = 0.7,
        max_tokens: int = 4096,
    ) -> str:
        """
        Generate text response from a prompt.
        """
        generation_config = genai.GenerationConfig(
            temperature=temperature,
            max_output_tokens=max_tokens,
        )

        try:
            response = self.model.generate_content(
                prompt,
                generation_config=generation_config,
            )
            return response.text
        except Exception as e:
            logger.error("Gemini generation error", error=str(e))
            return "ì£„ì†¡í•©ë‹ˆë‹¤. AI ëª¨ë¸ ì‘ë‹µ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

    def analyze_transcript(
        self,
        transcript: str,
        agenda: str | None = None,
    ) -> dict[str, Any]:
        """
        Analyze meeting transcript to extract decisions and action items.
        """
        agenda_section = f"íšŒì˜ ì•ˆê±´ì§€:\n{agenda}\n\n" if agenda else ""
        prompt = f"""ë‹¤ìŒ íšŒì˜ ì†ê¸°ë¡ì„ ë¶„ì„í•˜ì—¬ ê²°ì • ì‚¬í•­ê³¼ ì•¡ì…˜ ì•„ì´í…œì„ ì¶”ì¶œí•´ì£¼ì„¸ìš”.

{agenda_section}íšŒì˜ ì†ê¸°ë¡:
{transcript}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ JSON ì‘ë‹µì„ í•´ì£¼ì„¸ìš”:
{{
    "summary": "íšŒì˜ ìš”ì•½ (2-3ë¬¸ì¥)",
    "decisions": [
        {{"topic": "ë…¼ì˜ ì£¼ì œ", "decision": "ê²°ì • ë‚´ìš©"}}
    ],
    "action_items": [
        {{"task": "í•  ì¼", "assignee": "ë‹´ë‹¹ì (ì—†ìœ¼ë©´ null)", "due_date": "ë§ˆê°ì¼ (ì—†ìœ¼ë©´ null)"}}
    ]
}}
"""
        response_text = self.generate_text(prompt, temperature=0.3)
        result = self._parse_json_response(response_text)

        if not result:
            return {
                "summary": response_text,
                "decisions": [],
                "action_items": [],
            }
        return result

    def caption_image(
        self,
        image_bytes: bytes,
        mime_type: str = "image/jpeg",
    ) -> str:
        """
        Generate a caption/description for an image.
        """
        prompt = """ì´ ì´ë¯¸ì§€ê°€ í‘œë‚˜ ì¡°ì§ë„ë¼ë©´ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ êµ¬ì¡°ë¥¼ í…ìŠ¤íŠ¸í™”í•˜ê³ ,
ì¼ë°˜ ì‚¬ì§„ì´ë¼ë©´ ìƒí™©ì„ ìƒì„¸ ë¬˜ì‚¬í•´ ì¤˜. í•œêµ­ì–´ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”."""

        image_part = {
            "inline_data": {
                "mime_type": mime_type,
                "data": base64.b64encode(image_bytes).decode("utf-8"),
            }
        }

        try:
            response = self.vision_model.generate_content([prompt, image_part])
            return response.text
        except Exception as e:
            logger.error("Vision generation error", error=str(e))
            return "ì´ë¯¸ì§€ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

    def generate_answer(
        self,
        query: str,
        context: list[str],
        chat_history: str | None = None,
    ) -> str:
        """
        Generate an answer based on retrieved context (RAG).
        """
        context_text = "\n\n---\n\n".join(context) if context else "(ê²€ìƒ‰ëœ ë¬¸ì„œ ì—†ìŒ)"

        history_section = ""
        if chat_history and chat_history != "(ì´ì „ ëŒ€í™” ì—†ìŒ)":
            history_section = f"""
## ì´ì „ ëŒ€í™” (Context)
{chat_history}
"""

        prompt = f"""ë‹¹ì‹ ì€ í•™ìƒíšŒ ì—…ë¬´ë¥¼ ë•ëŠ” AI ë¹„ì„œ 'Council-AI'ì…ë‹ˆë‹¤.

## ì—­í• 
- ì œê³µëœ [ê²€ìƒ‰ëœ ë¬¸ì„œ]ë¥¼ ìµœìš°ì„  ê·¼ê±°ë¡œ ì‚¬ìš©í•˜ì—¬ ì •í™•í•˜ê²Œ ë‹µë³€í•©ë‹ˆë‹¤.
- ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ì•Šê³ , "í•´ë‹¹ ì •ë³´ë¥¼ ë¬¸ì„œì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µí•©ë‹ˆë‹¤.
- [ì´ì „ ëŒ€í™”]ì˜ ë§¥ë½ì„ ê³ ë ¤í•˜ì—¬, ì‚¬ìš©ìê°€ 'ê·¸ê±°', 'ì €ê±°'ë¡œ ì§€ì¹­í•œ ëŒ€ìƒì„ íŒŒì•…í•©ë‹ˆë‹¤.

## ê²€ìƒ‰ëœ ë¬¸ì„œ
{context_text}
{history_section}
## ì‚¬ìš©ì ì§ˆë¬¸
{query}

## ë‹µë³€ ê°€ì´ë“œë¼ì¸
1. ë‹µë³€ì€ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ë©°, ì¹œì ˆí•˜ê³  ì „ë¬¸ì ì¸ í†¤ì„ ìœ ì§€í•©ë‹ˆë‹¤.
2. í•µì‹¬ ê²°ë¡ ì„ ë‘ê´„ì‹ìœ¼ë¡œ ë¨¼ì € ì œì‹œí•©ë‹ˆë‹¤.
3. ì •ë³´ê°€ ë‚˜ì—´ë  ê²½ìš° ë§ˆí¬ë‹¤ìš´ ê¸€ë¨¸ë¦¬ ê¸°í˜¸ë‚˜ í‘œë¥¼ ì‚¬ìš©í•´ ê°€ë…ì„±ì„ ë†’ì…ë‹ˆë‹¤.
4. ì¶œì²˜ê°€ ëª…í™•í•œ ê²½ìš° "(ì¶œì²˜: ë¬¸ì„œëª…)"ê³¼ ê°™ì´ í‘œê¸°í•©ë‹ˆë‹¤.
5. ë‚ ì§œë‚˜ ì—°ë„ë¥¼ ë¬»ëŠ” ì§ˆë¬¸ì˜ ê²½ìš°, ë¬¸ì„œ ë‚´ìš©ì—ì„œ ë‚ ì§œ ì •ë³´(ì˜ˆ: "2025.05.01", "5ì›”", "ì œ37ëŒ€" ë“±)ë¥¼ ì ê·¹ì ìœ¼ë¡œ ì°¾ì•„ ë‹µë³€í•©ë‹ˆë‹¤.

## ë‹µë³€:"""

        # RAG ë‹µë³€ì€ ì‚¬ì‹¤ ê¸°ë°˜ì´ì–´ì•¼ í•˜ë¯€ë¡œ temperatureë¥¼ ë‚®ê²Œ ì„¤ì •
        # max_tokensë¥¼ 8192ë¡œ ëŠ˜ë ¤ ê¸´ ë‹µë³€ë„ ì˜ë¦¬ì§€ ì•Šê²Œ í•¨
        return self.generate_text(prompt, temperature=0.1, max_tokens=8192)

    def extract_calendar_events(self, text: str) -> list[dict[str, Any]]:
        """
        Extract calendar event information from text.
        """
        prompt = f"""ë‹¤ìŒ í…ìŠ¤íŠ¸ì—ì„œ ìº˜ë¦°ë”ì— ë“±ë¡í•  ì¼ì • ì •ë³´ë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”.

í…ìŠ¤íŠ¸:
{text}

ë‹¤ìŒ í˜•ì‹ì˜ JSON ë°°ì—´ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
[
    {{
        "title": "ì¼ì • ì œëª©",
        "date": "YYYY-MM-DD",
        "time": "HH:MM (ì—†ìœ¼ë©´ null)",
        "assignee": "ë‹´ë‹¹ì (ì—†ìœ¼ë©´ null)",
        "description": "ìƒì„¸ ë‚´ìš©"
    }}
]
"""
        response_text = self.generate_text(prompt, temperature=0.2)
        result = self._parse_json_response(response_text)

        return result if isinstance(result, list) else []

    # =========================================================================
    # Smart Minutes Feature Methods
    # =========================================================================

    def summarize_agenda_section(
        self,
        section_content: str,
        section_title: str,
        agenda_type: str = "discuss",
    ) -> dict[str, Any]:
        """
        Summarize a single agenda section from transcript for Smart Minutes.
        
        Args:
            section_content: Content of the agenda section (ë°œì–¸ ê¸°ë¡)
            section_title: Title of the agenda item
            agenda_type: Type of agenda (report, discuss, decision, other)
            
        Returns:
            Dict with 'summary', 'has_decision', 'action_items'
        """
        type_guidance = {
            "report": "ë³´ê³  ì•ˆê±´ì…ë‹ˆë‹¤. ì£¼ìš” ë³´ê³  ë‚´ìš©ì„ ê°„ëµíˆ ì •ë¦¬í•˜ì„¸ìš”.",
            "discuss": "ë…¼ì˜ ì•ˆê±´ì…ë‹ˆë‹¤. ê²°ì •ëœ ì‚¬í•­ì´ ìˆìœ¼ë©´ ëª…ì‹œí•˜ê³ , ì—†ìœ¼ë©´ 'ë…¼ì˜ ì§„í–‰ ì¤‘'ìœ¼ë¡œ í‘œì‹œí•˜ì„¸ìš”.",
            "decision": "ì˜ê²° ì•ˆê±´ì…ë‹ˆë‹¤. ì˜ê²° ê²°ê³¼(ê°€ê²°/ë¶€ê²°/ë³´ë¥˜)ë¥¼ ëª…í™•íˆ í‘œì‹œí•˜ì„¸ìš”.",
            "other": "ê¸°íƒ€ ì•ˆê±´ì…ë‹ˆë‹¤. í•µì‹¬ ë‚´ìš©ë§Œ ê°„ëµíˆ ìš”ì•½í•˜ì„¸ìš”.",
        }
        
        guidance = type_guidance.get(agenda_type, type_guidance["other"])
        
        prompt = f"""ë‹¹ì‹ ì€ í•™ìƒíšŒ íšŒì˜ë¡ ì‘ì„± ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

## ì•ˆê±´ ì •ë³´
- ì œëª©: {section_title}
- ìœ í˜•: {agenda_type} ({guidance})

## ì†ê¸° ë‚´ìš©
{section_content}

## ì‘ì—…
ìœ„ ì†ê¸° ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ [ê²°ê³¼ì§€]ì— ê¸°ì…í•  ë‚´ìš©ì„ ì‘ì„±í•˜ì„¸ìš”.

## ì¶œë ¥ í˜•ì‹ (JSON)
{{
    "summary": "ê²°ê³¼ì§€ì— ê¸°ì…í•  ìš”ì•½ (1-3ë¬¸ì¥, ê²°ë¡  ìœ„ì£¼)",
    "has_decision": true/false,
    "decisions": ["ê²°ì •ì‚¬í•­1", "ê²°ì •ì‚¬í•­2"],
    "action_items": [
        {{"task": "í•  ì¼", "assignee": "ë‹´ë‹¹ì ë˜ëŠ” null", "deadline": "ë§ˆê°ì¼ ë˜ëŠ” null"}}
    ],
    "discussion_progress": "ê²°ì • ì—†ì„ ì‹œ ë…¼ì˜ ì§„ì „ ìƒí™©"
}}

JSONë§Œ ì¶œë ¥í•˜ì„¸ìš”."""

        response_text = self.generate_text(prompt, temperature=0.2)
        result = self._parse_json_response(response_text)
        
        if not result:
            return {
                "summary": "ìš”ì•½ ìƒì„± ì‹¤íŒ¨",
                "has_decision": False,
                "decisions": [],
                "action_items": [],
                "discussion_progress": "",
            }
        
        return result

    def extract_todos_from_document(
        self,
        content: str,
        include_context: bool = True,
    ) -> list[dict[str, Any]]:
        """
        Extract todo/action items from a result document for Calendar Sync.
        
        Enhanced for Korean meeting transcripts (ëŒ€í™”í˜• ì†ê¸°ë¡).
        
        Args:
            content: Full text content of the result document
            include_context: Whether to include source context
            
        Returns:
            List of todo items with content, assignee, deadline, context
        """
        # Few-shot example for transcript-style content
        example_transcript = """ì˜ˆì‹œ ì…ë ¥:
í™ê¸¸ë™: ë‹¤ìŒ MT ì¥ì†ŒëŠ” ì œê°€ ì•Œì•„ë³¼ê²Œìš”. ì´ë²ˆ ì£¼ ê¸ˆìš”ì¼ê¹Œì§€ í›„ë³´ ì •ë¦¬í•´ì„œ ê³µìœ ë“œë¦¬ê² ìŠµë‹ˆë‹¤.
ê¹€ì² ìˆ˜: ë„¤, ê·¸ëŸ¼ ì˜ˆì‚°ì•ˆì€ ì œê°€ ì‘ì„±í• ê²Œìš”. ë‹¤ìŒ íšŒì˜ ì „ê¹Œì§€ ì´ˆì•ˆ ë§Œë“¤ì–´ë†“ì„ê²Œìš”.
"""
        example_output = """ì˜ˆì‹œ ì¶œë ¥:
[
    {"content": "MT ì¥ì†Œ í›„ë³´ ì¡°ì‚¬ ë° ì •ë¦¬", "context": "MT ê´€ë ¨ ë…¼ì˜", "assignee": "í™ê¸¸ë™", "suggested_date": "ì´ë²ˆ ì£¼ ê¸ˆìš”ì¼", "parsed_date": null},
    {"content": "ì˜ˆì‚°ì•ˆ ì´ˆì•ˆ ì‘ì„±", "context": "MT ê´€ë ¨ ë…¼ì˜", "assignee": "ê¹€ì² ìˆ˜", "suggested_date": "ë‹¤ìŒ íšŒì˜ ì „", "parsed_date": null}
]"""

        prompt = f"""ë‹¹ì‹ ì€ í•™ìƒíšŒ íšŒì˜ë¡ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì‚°ë°œì ì¸ ëŒ€í™” ì†ì—ì„œë„ 'í–‰ë™ì´ í•„ìš”í•œ ì‘ì—…(Action Item)'ì„ ì •í™•íˆ ì‹ë³„í•©ë‹ˆë‹¤.

## ë¶„ì„ ëŒ€ìƒ í…ìŠ¤íŠ¸
{content[:10000]}

## ì¶”ì¶œ ê¸°ì¤€
1. **ë°œí™”ì—ì„œ ìœ ì¶”**: "ì œê°€ í• ê²Œìš”", "ë§¡ê² ìŠµë‹ˆë‹¤", "ì•Œì•„ë³¼ê²Œìš”", "í™•ì¸í•´ë³´ê² ìŠµë‹ˆë‹¤" ë“±
2. **ëª…ì‹œì  ì§€ì‹œ**: "~í•´ì£¼ì„¸ìš”", "~ë¶€íƒë“œë¦½ë‹ˆë‹¤", "ë‹´ë‹¹: ëˆ„êµ¬"
3. **ë§ˆê° ì–¸ê¸‰**: "ì–¸ì œê¹Œì§€", "ë‹¤ìŒ ì£¼", "ê¸ˆìš”ì¼", "íšŒì˜ ì „ê¹Œì§€"
4. **ê²°ì • ì‚¬í•­**: "~ë¡œ ê²°ì •", "~í•˜ê¸°ë¡œ í•¨" (ì´ê²ƒë„ í›„ì† ì¡°ì¹˜ê°€ í•„ìš”í•˜ë©´ ì¶”ì¶œ)

*ì¤‘ìš”: ëŒ€í™”í˜• ì†ê¸°ë¡ì—ì„œë„ ë°œí™”ìì˜ ì•½ì†ì´ë‚˜ ì˜ì§€ í‘œí˜„ì„ Action Itemìœ¼ë¡œ ì¸ì‹í•˜ì„¸ìš”.*
*í•  ì¼ì´ ì „í˜€ ì—†ì–´ ë³´ì—¬ë„, íšŒì˜ì—ì„œ ë…¼ì˜ëœ í›„ì† ì¡°ì¹˜ê°€ ìˆë‹¤ë©´ ì¶”ì¶œí•˜ì„¸ìš”.*

## Few-shot ì˜ˆì‹œ
{example_transcript}
{example_output}

## ì‹¤ì œ ë¶„ì„ ëŒ€ìƒ
ìœ„ í…ìŠ¤íŠ¸ì—ì„œ í•  ì¼(Action Item)ì„ ì¶”ì¶œí•˜ì„¸ìš”.

## ì¶œë ¥ í˜•ì‹ (JSON ë°°ì—´ë§Œ ì¶œë ¥)
[
    {{
        "content": "êµ¬ì²´ì ì¸ í•  ì¼ ë‚´ìš©",
        "context": "ê´€ë ¨ ì•ˆê±´ ë˜ëŠ” ë°œì–¸ ë§¥ë½",
        "assignee": "ë‹´ë‹¹ì ì´ë¦„/ì§ì±… (ì—†ìœ¼ë©´ null)",
        "suggested_date": "ì–¸ê¸‰ëœ ë§ˆê°ì¼ í…ìŠ¤íŠ¸ (ì—†ìœ¼ë©´ null)",
        "parsed_date": "YYYY-MM-DD í˜•ì‹ (íŒŒì‹± ë¶ˆê°€ ì‹œ null)"
    }}
]

ë°˜ë“œì‹œ JSON ë°°ì—´ë§Œ ì¶œë ¥í•˜ì„¸ìš”. ì„¤ëª…ì´ë‚˜ ì£¼ì„ì„ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”.
í•  ì¼ì´ ì—†ìœ¼ë©´ ë¹ˆ ë°°ì—´ []ì„ ì¶œë ¥í•˜ì„¸ìš”."""

        response_text = self.generate_text(prompt, temperature=0.2)
        
        # Debug logging for response analysis
        logger.debug(
            "Gemini todo extraction response",
            response_preview=response_text[:500] if response_text else "(empty)",
            response_length=len(response_text) if response_text else 0,
        )
        
        result = self._parse_json_response(response_text)
        
        # Additional logging for parsing result
        if not isinstance(result, list):
            logger.warning(
                "Todo extraction returned non-list",
                result_type=type(result).__name__,
                response_preview=response_text[:300] if response_text else "(empty)",
            )
            return []
        
        logger.info(
            "Todo extraction parsed successfully",
            todos_count=len(result),
        )
        
        return result

    def generate_handover_insight(
        self,
        event_title: str,
        event_content: str,
    ) -> dict[str, Any]:
        """
        Generate deep analysis for a single event based on its document content.
        
        This function reads actual meeting transcripts, agendas, and results
        to produce strategic insights for the next student council.
        
        Args:
            event_title: Title of the event/project
            event_content: Aggregated preprocessed_content from related documents
            
        Returns:
            Dict with keys: overview, key_decisions, success_points,
                            improvement_points, next_year_advice
        """
        # Limit content to prevent context overflow
        content_truncated = event_content[:15000] if event_content else "(ë¬¸ì„œ ë‚´ìš© ì—†ìŒ)"
        
        prompt = f"""ë‹¹ì‹ ì€ í•™ìƒíšŒ ì¸ìˆ˜ì¸ê³„ ë‹´ë‹¹ìì…ë‹ˆë‹¤.
í›„ë°° í•™ìƒíšŒê°€ ë‚´ë…„ì— ì´ í–‰ì‚¬ë¥¼ ë” ì˜ ìš´ì˜í•  ìˆ˜ ìˆë„ë¡ ë¶„ì„í•´ì£¼ì„¸ìš”.

## í–‰ì‚¬ëª…
{event_title}

## ê´€ë ¨ ë¬¸ì„œ ë‚´ìš© (íšŒì˜ë¡, ì•ˆê±´ì§€, ê²°ê³¼ì§€ ë“±)
{content_truncated}

## ë¶„ì„ ìš”ì²­
ìœ„ ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ í•­ëª©ì„ ë¶„ì„í•˜ì„¸ìš”:
1. í–‰ì‚¬ ê°œìš” (ì–¸ì œ, ì–´ë””ì„œ, ë¬´ì—‡ì„)
2. ì£¼ìš” ê²°ì •ì‚¬í•­ (êµ¬ì²´ì ì¸ íŒ©íŠ¸ ìœ„ì£¼)
3. ì˜í•œ ì  (ì„±ê³µ ìš”ì¸)
4. ì•„ì‰¬ìš´ ì  / ê°œì„  í•„ìš” ì‚¬í•­
5. ë‚´ë…„ ë‹´ë‹¹ìë¥¼ ìœ„í•œ êµ¬ì²´ì ì¸ ì¡°ì–¸

## ì¶œë ¥ í˜•ì‹ (JSON)
{{
    "overview": "í–‰ì‚¬ ê°œìš” ìš”ì•½ (1-2ë¬¸ì¥)",
    "key_decisions": ["ì£¼ìš” ê²°ì •ì‚¬í•­1", "ì£¼ìš” ê²°ì •ì‚¬í•­2"],
    "success_points": ["ì˜í•œ ì 1", "ì˜í•œ ì 2"],
    "improvement_points": ["ì•„ì‰¬ìš´ ì 1", "ê°œì„  í•„ìš” ì‚¬í•­2"],
    "next_year_advice": "ë‚´ë…„ ë‹´ë‹¹ìë¥¼ ìœ„í•œ êµ¬ì²´ì ì¸ ì¡°ì–¸ (3-5ë¬¸ì¥)"
}}

JSONë§Œ ì¶œë ¥í•˜ì„¸ìš”. ë¬¸ì„œì— ì •ë³´ê°€ ë¶€ì¡±í•˜ë©´ í•´ë‹¹ í•­ëª©ì€ ë¹ˆ ë°°ì—´ì´ë‚˜ "(ì •ë³´ ë¶€ì¡±)"ìœ¼ë¡œ í‘œì‹œí•˜ì„¸ìš”."""

        response_text = self.generate_text(prompt, temperature=0.3)
        result = self._parse_json_response(response_text)
        
        if not result:
            return {
                "overview": "(ë¶„ì„ ì‹¤íŒ¨)",
                "key_decisions": [],
                "success_points": [],
                "improvement_points": [],
                "next_year_advice": "(ë¶„ì„ ì‹¤íŒ¨)",
            }
        
        return result

    def generate_handover_content(
        self,
        events_data: list[dict[str, Any]],
        year: int,
        department: str | None = None,
        include_insights: bool = True,
    ) -> str:
        """
        Generate comprehensive handover document content.
        
        Args:
            events_data: List of event dictionaries with title, date, summary, etc.
            year: Target year
            department: Optional department filter
            include_insights: Whether to include AI insights
            
        Returns:
            Markdown formatted handover content
        """
        dept_text = f"{department} " if department else ""
        
        # Format events for prompt
        events_text = ""
        for event in events_data[:30]:  # Limit to prevent context overflow
            events_text += f"""
### {event.get('title', 'ì œëª© ì—†ìŒ')}
- ë‚ ì§œ: {event.get('event_date', 'ë¯¸ì •')}
- ë‹´ë‹¹: {event.get('category', 'ë¯¸ì •')}
- ìƒíƒœ: {event.get('status', 'ë¯¸ì •')}
- ìš”ì•½: {event.get('summary', '(ìš”ì•½ ì—†ìŒ)')}
"""
        
        insights_instruction = """
## 7. ì°¨ê¸° í•™ìƒíšŒë¥¼ ìœ„í•œ ì œì–¸
- ì „ì²´ ì‚¬ì—… ìš´ì˜ì— ëŒ€í•œ ì¸ì‚¬ì´íŠ¸
- ê°œì„ ì´ í•„ìš”í•œ ë¶€ë¶„
- ìœ ì§€í•˜ë©´ ì¢‹ì„ ê²ƒë“¤
""" if include_insights else ""
        
        prompt = f"""ë‹¹ì‹ ì€ í•™ìƒíšŒ ì¸ìˆ˜ì¸ê³„ì„œ ì‘ì„± ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

## ì‘ì„± ëŒ€ìƒ
- ì—°ë„: {year}ë…„
- ëŒ€ìƒ: {dept_text}í•™ìƒíšŒ

## í–‰ì‚¬/ì‚¬ì—… ë°ì´í„°
{events_text}

## ì¸ìˆ˜ì¸ê³„ì„œ êµ¬ì¡°
ë‹¤ìŒ êµ¬ì¡°ì— ë§ì¶° Markdown í˜•ì‹ì˜ ì¸ìˆ˜ì¸ê³„ì„œë¥¼ ì‘ì„±í•˜ì„¸ìš”:

# ì œ38ëŒ€ {dept_text}í•™ìƒíšŒ ì¸ìˆ˜ì¸ê³„ì„œ ({year})

## 1. ê°œìš”
- {year}ë…„ í•™ìƒíšŒ í™œë™ ì „ë°˜ì— ëŒ€í•œ ì†Œê°œ

## 2. ì¡°ì§ êµ¬ì„±
- ì£¼ìš” ë³´ì§ ë° ë‹´ë‹¹ ì—…ë¬´

## 3. ì£¼ìš” ì‚¬ì—… ì´ê´„
- ì—°ê°„ ì‚¬ì—… íƒ€ì„ë¼ì¸
- ì£¼ìš” ì„±ê³¼

## 4. ì‚¬ì—…ë³„ ìƒì„¸ ê¸°ë¡
(ê° í–‰ì‚¬ì— ëŒ€í•œ ê¸°íš ì˜ë„, ì§„í–‰ ê³¼ì •, ê²°ê³¼, í”¼ë“œë°±)

## 5. ì˜ˆì‚° ìš´ìš© ê°œìš”
- ì£¼ìš” ì§€ì¶œ í•­ëª©
- ì˜ˆì‚° ê´€ë¦¬ íŒ

## 6. ì£¼ìš” ê²°ì •ì‚¬í•­ ì•„ì¹´ì´ë¸Œ
- ì¤‘ìš”í•œ ì˜ì‚¬ê²°ì • ê¸°ë¡
{insights_instruction}

ìœ„ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‹¤ì§ˆì ìœ¼ë¡œ ë„ì›€ì´ ë˜ëŠ” ì¸ìˆ˜ì¸ê³„ì„œë¥¼ ì‘ì„±í•˜ì„¸ìš”.
ì—†ëŠ” ì •ë³´ëŠ” ì¶”ì¸¡í•˜ì§€ ë§ê³  "(ì •ë³´ ì—†ìŒ)" ë˜ëŠ” "(ì¶”ê°€ í•„ìš”)"ë¡œ í‘œì‹œí•˜ì„¸ìš”.
"""

        return self.generate_text(prompt, temperature=0.4, max_tokens=8000)
