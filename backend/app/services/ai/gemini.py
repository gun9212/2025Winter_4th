"""Gemini AI service for LLM and Vision operations."""

import base64
import json
from typing import Any

import google.generativeai as genai
import structlog

from app.core.config import settings

logger = structlog.get_logger()


class GeminiService:
    """Service for Gemini LLM and Vision capabilities."""

    def __init__(self) -> None:
        # Vertex AIê°€ ì•„ë‹Œ Google AI Studio API í‚¤ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš° configure í•„ìš”
        # Vertex AI í™˜ê²½(GCP)ì´ë¼ë©´ ì´ˆê¸°í™” ë°©ì‹ì´ ë‹¤ë¥¼ ìˆ˜ ìˆìœ¼ë‚˜,
        # í˜„ì¬ ì½”ë“œ ë² ì´ìŠ¤ëŠ” api_key ë°©ì‹ì„ ë”°ë¦…ë‹ˆë‹¤.
        genai.configure(api_key=settings.GEMINI_API_KEY)
        self._model = None
        self._vision_model = None

        # ğŸš€ [Upgrade] ìµœì‹  Gemini 2.0 ëª¨ë¸ ì‚¬ìš©
        # ë§Œì•½ ì—ëŸ¬ ë°œìƒ ì‹œ "gemini-1.5-flash-001"ë¡œ ë³€ê²½í•˜ì„¸ìš”.
        self.MODEL_NAME = "gemini-flash-latest"

    @property
    def model(self):
        """Get text generation model."""
        if self._model is None:
            self._model = genai.GenerativeModel(self.MODEL_NAME)
        return self._model

    @property
    def vision_model(self):
        """Get vision-capable model."""
        if self._vision_model is None:
            self._vision_model = genai.GenerativeModel(self.MODEL_NAME)
        return self._vision_model

    def _parse_json_response(self, response_text: str) -> dict | list:
        """Helper to cleanly parse JSON from LLM response."""
        try:
            json_str = response_text
            if "```json" in response_text:
                json_str = response_text.split("```json")[1].split("```")[0]
            elif "```" in response_text:
                json_str = response_text.split("```")[1].split("```")[0]

            return json.loads(json_str.strip())
        except (json.JSONDecodeError, IndexError):
            # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì›ë³¸ í…ìŠ¤íŠ¸ë¥¼ í¬í•¨í•œ ì—ëŸ¬ êµ¬ì¡° ë°˜í™˜ ë˜ëŠ” ë¹ˆ ê°’ ë°˜í™˜
            return {}

    def generate_text(
        self,
        prompt: str,
        temperature: float = 0.7,
        max_tokens: int = 4096,
    ) -> str:
        """
        Generate text response from a prompt.
        """
        generation_config = genai.GenerationConfig(
            temperature=temperature,
            max_output_tokens=max_tokens,
        )

        try:
            response = self.model.generate_content(
                prompt,
                generation_config=generation_config,
            )
            return response.text
        except Exception as e:
            logger.error("Gemini generation error", error=str(e))
            return "ì£„ì†¡í•©ë‹ˆë‹¤. AI ëª¨ë¸ ì‘ë‹µ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

    def analyze_transcript(
        self,
        transcript: str,
        agenda: str | None = None,
    ) -> dict[str, Any]:
        """
        Analyze meeting transcript to extract decisions and action items.
        """
        agenda_section = f"íšŒì˜ ì•ˆê±´ì§€:\n{agenda}\n\n" if agenda else ""
        prompt = f"""ë‹¤ìŒ íšŒì˜ ì†ê¸°ë¡ì„ ë¶„ì„í•˜ì—¬ ê²°ì • ì‚¬í•­ê³¼ ì•¡ì…˜ ì•„ì´í…œì„ ì¶”ì¶œí•´ì£¼ì„¸ìš”.

{agenda_section}íšŒì˜ ì†ê¸°ë¡:
{transcript}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ JSON ì‘ë‹µì„ í•´ì£¼ì„¸ìš”:
{{
    "summary": "íšŒì˜ ìš”ì•½ (2-3ë¬¸ì¥)",
    "decisions": [
        {{"topic": "ë…¼ì˜ ì£¼ì œ", "decision": "ê²°ì • ë‚´ìš©"}}
    ],
    "action_items": [
        {{"task": "í•  ì¼", "assignee": "ë‹´ë‹¹ì (ì—†ìœ¼ë©´ null)", "due_date": "ë§ˆê°ì¼ (ì—†ìœ¼ë©´ null)"}}
    ]
}}
"""
        response_text = self.generate_text(prompt, temperature=0.3)
        result = self._parse_json_response(response_text)

        if not result:
            return {
                "summary": response_text,
                "decisions": [],
                "action_items": [],
            }
        return result

    def caption_image(
        self,
        image_bytes: bytes,
        mime_type: str = "image/jpeg",
    ) -> str:
        """
        Generate a caption/description for an image.
        """
        prompt = """ì´ ì´ë¯¸ì§€ê°€ í‘œë‚˜ ì¡°ì§ë„ë¼ë©´ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ êµ¬ì¡°ë¥¼ í…ìŠ¤íŠ¸í™”í•˜ê³ ,
ì¼ë°˜ ì‚¬ì§„ì´ë¼ë©´ ìƒí™©ì„ ìƒì„¸ ë¬˜ì‚¬í•´ ì¤˜. í•œêµ­ì–´ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”."""

        image_part = {
            "inline_data": {
                "mime_type": mime_type,
                "data": base64.b64encode(image_bytes).decode("utf-8"),
            }
        }

        try:
            response = self.vision_model.generate_content([prompt, image_part])
            return response.text
        except Exception as e:
            logger.error("Vision generation error", error=str(e))
            return "ì´ë¯¸ì§€ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

    def generate_answer(
        self,
        query: str,
        context: list[str],
        chat_history: str | None = None,
    ) -> str:
        """
        Generate an answer based on retrieved context (RAG).
        """
        context_text = "\n\n---\n\n".join(context) if context else "(ê²€ìƒ‰ëœ ë¬¸ì„œ ì—†ìŒ)"

        history_section = ""
        if chat_history and chat_history != "(ì´ì „ ëŒ€í™” ì—†ìŒ)":
            history_section = f"""
## ì´ì „ ëŒ€í™” (Context)
{chat_history}
"""

        prompt = f"""ë‹¹ì‹ ì€ í•™ìƒíšŒ ì—…ë¬´ë¥¼ ë•ëŠ” AI ë¹„ì„œ 'Council-AI'ì…ë‹ˆë‹¤.

## ì—­í• 
- ì œê³µëœ [ê²€ìƒ‰ëœ ë¬¸ì„œ]ë¥¼ ìµœìš°ì„  ê·¼ê±°ë¡œ ì‚¬ìš©í•˜ì—¬ ì •í™•í•˜ê²Œ ë‹µë³€í•©ë‹ˆë‹¤.
- ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ì•Šê³ , "í•´ë‹¹ ì •ë³´ë¥¼ ë¬¸ì„œì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µí•©ë‹ˆë‹¤.
- [ì´ì „ ëŒ€í™”]ì˜ ë§¥ë½ì„ ê³ ë ¤í•˜ì—¬, ì‚¬ìš©ìê°€ 'ê·¸ê±°', 'ì €ê±°'ë¡œ ì§€ì¹­í•œ ëŒ€ìƒì„ íŒŒì•…í•©ë‹ˆë‹¤.

## ê²€ìƒ‰ëœ ë¬¸ì„œ
{context_text}
{history_section}
## ì‚¬ìš©ì ì§ˆë¬¸
{query}

## ë‹µë³€ ê°€ì´ë“œë¼ì¸
1. ë‹µë³€ì€ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ë©°, ì¹œì ˆí•˜ê³  ì „ë¬¸ì ì¸ í†¤ì„ ìœ ì§€í•©ë‹ˆë‹¤.
2. í•µì‹¬ ê²°ë¡ ì„ ë‘ê´„ì‹ìœ¼ë¡œ ë¨¼ì € ì œì‹œí•©ë‹ˆë‹¤.
3. ì •ë³´ê°€ ë‚˜ì—´ë  ê²½ìš° ë§ˆí¬ë‹¤ìš´ ê¸€ë¨¸ë¦¬ ê¸°í˜¸ë‚˜ í‘œë¥¼ ì‚¬ìš©í•´ ê°€ë…ì„±ì„ ë†’ì…ë‹ˆë‹¤.
4. ì¶œì²˜ê°€ ëª…í™•í•œ ê²½ìš° "(ì¶œì²˜: ë¬¸ì„œëª…)"ê³¼ ê°™ì´ í‘œê¸°í•©ë‹ˆë‹¤.
5. ë‚ ì§œë‚˜ ì—°ë„ë¥¼ ë¬»ëŠ” ì§ˆë¬¸ì˜ ê²½ìš°, ë¬¸ì„œ ë‚´ìš©ì—ì„œ ë‚ ì§œ ì •ë³´(ì˜ˆ: "2025.05.01", "5ì›”", "ì œ37ëŒ€" ë“±)ë¥¼ ì ê·¹ì ìœ¼ë¡œ ì°¾ì•„ ë‹µë³€í•©ë‹ˆë‹¤.

## ë‹µë³€:"""

        # RAG ë‹µë³€ì€ ì‚¬ì‹¤ ê¸°ë°˜ì´ì–´ì•¼ í•˜ë¯€ë¡œ temperatureë¥¼ ë‚®ê²Œ ì„¤ì •
        # max_tokensë¥¼ 8192ë¡œ ëŠ˜ë ¤ ê¸´ ë‹µë³€ë„ ì˜ë¦¬ì§€ ì•Šê²Œ í•¨
        return self.generate_text(prompt, temperature=0.1, max_tokens=8192)

    def extract_calendar_events(self, text: str) -> list[dict[str, Any]]:
        """
        Extract calendar event information from text.
        """
        prompt = f"""ë‹¤ìŒ í…ìŠ¤íŠ¸ì—ì„œ ìº˜ë¦°ë”ì— ë“±ë¡í•  ì¼ì • ì •ë³´ë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”.

í…ìŠ¤íŠ¸:
{text}

ë‹¤ìŒ í˜•ì‹ì˜ JSON ë°°ì—´ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
[
    {{
        "title": "ì¼ì • ì œëª©",
        "date": "YYYY-MM-DD",
        "time": "HH:MM (ì—†ìœ¼ë©´ null)",
        "assignee": "ë‹´ë‹¹ì (ì—†ìœ¼ë©´ null)",
        "description": "ìƒì„¸ ë‚´ìš©"
    }}
]
"""
        response_text = self.generate_text(prompt, temperature=0.2)
        result = self._parse_json_response(response_text)

        return result if isinstance(result, list) else []

    # =========================================================================
    # Smart Minutes Feature Methods
    # =========================================================================

    def summarize_agenda_section(
        self,
        section_content: str,
        section_title: str,
        agenda_type: str = "discuss",
    ) -> dict[str, Any]:
        """
        Summarize a single agenda section from transcript for Smart Minutes.
        
        Args:
            section_content: Content of the agenda section (ë°œì–¸ ê¸°ë¡)
            section_title: Title of the agenda item
            agenda_type: Type of agenda (report, discuss, decision, other)
            
        Returns:
            Dict with 'summary', 'has_decision', 'action_items'
        """
        type_guidance = {
            "report": "ë³´ê³  ì•ˆê±´ì…ë‹ˆë‹¤. ì£¼ìš” ë³´ê³  ë‚´ìš©ì„ ê°„ëµíˆ ì •ë¦¬í•˜ì„¸ìš”.",
            "discuss": "ë…¼ì˜ ì•ˆê±´ì…ë‹ˆë‹¤. ê²°ì •ëœ ì‚¬í•­ì´ ìˆìœ¼ë©´ ëª…ì‹œí•˜ê³ , ì—†ìœ¼ë©´ 'ë…¼ì˜ ì§„í–‰ ì¤‘'ìœ¼ë¡œ í‘œì‹œí•˜ì„¸ìš”.",
            "decision": "ì˜ê²° ì•ˆê±´ì…ë‹ˆë‹¤. ì˜ê²° ê²°ê³¼(ê°€ê²°/ë¶€ê²°/ë³´ë¥˜)ë¥¼ ëª…í™•íˆ í‘œì‹œí•˜ì„¸ìš”.",
            "other": "ê¸°íƒ€ ì•ˆê±´ì…ë‹ˆë‹¤. í•µì‹¬ ë‚´ìš©ë§Œ ê°„ëµíˆ ìš”ì•½í•˜ì„¸ìš”.",
        }
        
        guidance = type_guidance.get(agenda_type, type_guidance["other"])
        
        prompt = f"""ë‹¹ì‹ ì€ í•™ìƒíšŒ íšŒì˜ë¡ ì‘ì„± ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

## ì•ˆê±´ ì •ë³´
- ì œëª©: {section_title}
- ìœ í˜•: {agenda_type} ({guidance})

## ì†ê¸° ë‚´ìš©
{section_content}

## ì‘ì—…
ìœ„ ì†ê¸° ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ [ê²°ê³¼ì§€]ì— ê¸°ì…í•  ë‚´ìš©ì„ ì‘ì„±í•˜ì„¸ìš”.

## ì¶œë ¥ í˜•ì‹ (JSON)
{{
    "summary": "ê²°ê³¼ì§€ì— ê¸°ì…í•  ìš”ì•½ (1-3ë¬¸ì¥, ê²°ë¡  ìœ„ì£¼)",
    "has_decision": true/false,
    "decisions": ["ê²°ì •ì‚¬í•­1", "ê²°ì •ì‚¬í•­2"],
    "action_items": [
        {{"task": "í•  ì¼", "assignee": "ë‹´ë‹¹ì ë˜ëŠ” null", "deadline": "ë§ˆê°ì¼ ë˜ëŠ” null"}}
    ],
    "discussion_progress": "ê²°ì • ì—†ì„ ì‹œ ë…¼ì˜ ì§„ì „ ìƒí™©"
}}

JSONë§Œ ì¶œë ¥í•˜ì„¸ìš”."""

        response_text = self.generate_text(prompt, temperature=0.2)
        result = self._parse_json_response(response_text)
        
        if not result:
            return {
                "summary": "ìš”ì•½ ìƒì„± ì‹¤íŒ¨",
                "has_decision": False,
                "decisions": [],
                "action_items": [],
                "discussion_progress": "",
            }
        
        return result

    def extract_todos_from_document(
        self,
        content: str,
        include_context: bool = True,
    ) -> list[dict[str, Any]]:
        """
        Extract todo/action items from a result document for Calendar Sync.
        
        Args:
            content: Full text content of the result document
            include_context: Whether to include source context
            
        Returns:
            List of todo items with content, assignee, deadline, context
        """
        prompt = f"""ë‹¤ìŒ íšŒì˜ ê²°ê³¼ ë¬¸ì„œì—ì„œ í•´ì•¼ í•  ì¼(Todo/Action Item)ì„ ì¶”ì¶œí•´ì£¼ì„¸ìš”.

## íšŒì˜ ê²°ê³¼ ë¬¸ì„œ
{content[:8000]}  # Limit context length

## ì¶”ì¶œ ê¸°ì¤€
1. ëª…ì‹œì ì¸ í•  ì¼: "~ì˜ˆì •", "~ì§„í–‰", "~ì™„ë£Œ í•„ìš”"
2. ë‹´ë‹¹ì ì§€ì •: "ë‹´ë‹¹:", "ë‹´ë‹¹ì:", ë¶€ì„œëª… ì–¸ê¸‰
3. ë§ˆê°ì¼ ì–¸ê¸‰: "~ê¹Œì§€", "~ì¼", "ë‹¤ìŒ ì£¼"

## ì¶œë ¥ í˜•ì‹ (JSON)
[
    {{
        "content": "í•  ì¼ ë‚´ìš©",
        "context": "ì–´ëŠ ì•ˆê±´ì—ì„œ ë‚˜ì˜¨ ê²ƒì¸ì§€ (ì˜ˆ: ë¬¸í™”êµ­ ë³´ê³ )",
        "assignee": "ë‹´ë‹¹ì ë˜ëŠ” ë‹´ë‹¹ ë¶€ì„œ (ì—†ìœ¼ë©´ null)",
        "suggested_date": "ë¬¸ì„œì— ì–¸ê¸‰ëœ ë‚ ì§œ í…ìŠ¤íŠ¸ (ì—†ìœ¼ë©´ null)",
        "parsed_date": "YYYY-MM-DD í˜•ì‹ìœ¼ë¡œ íŒŒì‹±ëœ ë‚ ì§œ (íŒŒì‹± ë¶ˆê°€ ì‹œ null)"
    }}
]

JSON ë°°ì—´ë§Œ ì¶œë ¥í•˜ì„¸ìš”."""

        response_text = self.generate_text(prompt, temperature=0.2)
        result = self._parse_json_response(response_text)
        
        return result if isinstance(result, list) else []

    def generate_handover_content(
        self,
        events_data: list[dict[str, Any]],
        year: int,
        department: str | None = None,
        include_insights: bool = True,
    ) -> str:
        """
        Generate comprehensive handover document content.
        
        Args:
            events_data: List of event dictionaries with title, date, summary, etc.
            year: Target year
            department: Optional department filter
            include_insights: Whether to include AI insights
            
        Returns:
            Markdown formatted handover content
        """
        dept_text = f"{department} " if department else ""
        
        # Format events for prompt
        events_text = ""
        for event in events_data[:30]:  # Limit to prevent context overflow
            events_text += f"""
### {event.get('title', 'ì œëª© ì—†ìŒ')}
- ë‚ ì§œ: {event.get('event_date', 'ë¯¸ì •')}
- ë‹´ë‹¹: {event.get('category', 'ë¯¸ì •')}
- ìƒíƒœ: {event.get('status', 'ë¯¸ì •')}
- ìš”ì•½: {event.get('summary', '(ìš”ì•½ ì—†ìŒ)')}
"""
        
        insights_instruction = """
## 7. ì°¨ê¸° í•™ìƒíšŒë¥¼ ìœ„í•œ ì œì–¸
- ì „ì²´ ì‚¬ì—… ìš´ì˜ì— ëŒ€í•œ ì¸ì‚¬ì´íŠ¸
- ê°œì„ ì´ í•„ìš”í•œ ë¶€ë¶„
- ìœ ì§€í•˜ë©´ ì¢‹ì„ ê²ƒë“¤
""" if include_insights else ""
        
        prompt = f"""ë‹¹ì‹ ì€ í•™ìƒíšŒ ì¸ìˆ˜ì¸ê³„ì„œ ì‘ì„± ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

## ì‘ì„± ëŒ€ìƒ
- ì—°ë„: {year}ë…„
- ëŒ€ìƒ: {dept_text}í•™ìƒíšŒ

## í–‰ì‚¬/ì‚¬ì—… ë°ì´í„°
{events_text}

## ì¸ìˆ˜ì¸ê³„ì„œ êµ¬ì¡°
ë‹¤ìŒ êµ¬ì¡°ì— ë§ì¶° Markdown í˜•ì‹ì˜ ì¸ìˆ˜ì¸ê³„ì„œë¥¼ ì‘ì„±í•˜ì„¸ìš”:

# ì œ38ëŒ€ {dept_text}í•™ìƒíšŒ ì¸ìˆ˜ì¸ê³„ì„œ ({year})

## 1. ê°œìš”
- {year}ë…„ í•™ìƒíšŒ í™œë™ ì „ë°˜ì— ëŒ€í•œ ì†Œê°œ

## 2. ì¡°ì§ êµ¬ì„±
- ì£¼ìš” ë³´ì§ ë° ë‹´ë‹¹ ì—…ë¬´

## 3. ì£¼ìš” ì‚¬ì—… ì´ê´„
- ì—°ê°„ ì‚¬ì—… íƒ€ì„ë¼ì¸
- ì£¼ìš” ì„±ê³¼

## 4. ì‚¬ì—…ë³„ ìƒì„¸ ê¸°ë¡
(ê° í–‰ì‚¬ì— ëŒ€í•œ ê¸°íš ì˜ë„, ì§„í–‰ ê³¼ì •, ê²°ê³¼, í”¼ë“œë°±)

## 5. ì˜ˆì‚° ìš´ìš© ê°œìš”
- ì£¼ìš” ì§€ì¶œ í•­ëª©
- ì˜ˆì‚° ê´€ë¦¬ íŒ

## 6. ì£¼ìš” ê²°ì •ì‚¬í•­ ì•„ì¹´ì´ë¸Œ
- ì¤‘ìš”í•œ ì˜ì‚¬ê²°ì • ê¸°ë¡
{insights_instruction}

ìœ„ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‹¤ì§ˆì ìœ¼ë¡œ ë„ì›€ì´ ë˜ëŠ” ì¸ìˆ˜ì¸ê³„ì„œë¥¼ ì‘ì„±í•˜ì„¸ìš”.
ì—†ëŠ” ì •ë³´ëŠ” ì¶”ì¸¡í•˜ì§€ ë§ê³  "(ì •ë³´ ì—†ìŒ)" ë˜ëŠ” "(ì¶”ê°€ í•„ìš”)"ë¡œ í‘œì‹œí•˜ì„¸ìš”.
"""

        return self.generate_text(prompt, temperature=0.4, max_tokens=8000)
